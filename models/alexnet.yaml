---
dataset:
    background_class: {use: false}
    preprocess:
        shape:
            height: 224
            width: 224
            channels: 3
        validate: []
        final:
            - {type: normalize_channels}
model:
    name: alexnet
    description: |
        This AlexNet implementation is based on::
            One weird trick for parallelizing convolutional neural networks
            (Alex Krizhevsky, 2014)

        We use the following reference models::
            https://github.com/tensorflow/models/blob/master/slim/nets/alexnet.py
            https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py

        Unfortunately there are some discrepancies between the above models,
        namely `conv4` uses different output sizes (384 vs. 256), and padding
        of `conv1` is different (valid vs. same).
    layers:
        _init: &init
            biases_initializer:
                type: tensorflow.constant_initializer
                value: 0.01
        _conv: &conv
            type: convolution
            padding: same
            # weight_initializer defaults to xavier
            weights_regularizer:
                type: tensorflow.contrib.layers.l2_regularizer
                scale: 0.0005
            <<: *init
        _fc: &fc
            type: fully_connected
            weights_initializer:
                type: tensorflow.truncated_normal_initializer
                stddev: 0.09
            <<: *init
        conv1: {<<: *conv, kernel_size: 11, stride: 4, num_outputs: 64}
        pool1: &pool
            {type: max_pool, kernel_size: 3, stride: 2, padding: valid}
        conv2: {<<: *conv, kernel_size: 5, stride: 1, num_outputs: 192}
        pool2: *pool
        conv3: &conv3
            {<<: *conv, kernel_size: 3, stride: 1, num_outputs: 384}
        conv4: &conv4 {<<: *conv3, num_outputs: 256}
        conv5: *conv4
        pool5: *pool
        dropout5: {type: dropout, keep_prob: 0.5}
        flatten5: {type: flatten}
        fc6: {<<: *fc, num_outputs: 4096}
        dropout6: &dropout {type: dropout, keep_prob: 0.5}
        fc7: {<<: *fc, num_outputs: 4096}
        # dropout7: *dropout
        logits:
            <<: *fc
            num_outputs: num_classes
            activation_fn: null
            biases_initializer: {type: tensorflow.zeros_initializer}
    net: [
        conv1, pool1, conv2, pool2, conv3, conv4, conv5, pool5,
        flatten5, fc6, dropout6, fc7, dropout7, logits]
